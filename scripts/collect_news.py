#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
- ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (Google ë°±ì—…)
- ì¢…ëª©ë‹¹ ìµœê·¼ ë‰´ìŠ¤ 5ê°œ ìˆ˜ì§‘
"""

import os
import sys
import time
import random
from datetime import datetime, timedelta
from pathlib import Path

import requests
from bs4 import BeautifulSoup

sys.path.insert(0, str(Path(__file__).parent.parent / "backend"))


USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
]


def get_stock_name(stock_code: str) -> str:
    """ì¢…ëª©ì½”ë“œ â†’ ì¢…ëª©ëª… ë³€í™˜"""
    # TODO: Supabaseì—ì„œ ì¡°íšŒ
    stock_names = {
        "138040": "ë©”ë¦¬ì¸ ê¸ˆìœµì§€ì£¼",
        "005930": "ì‚¼ì„±ì „ì",
        "383220": "F&F",
        "259960": "í¬ë˜í”„í†¤",
        "271560": "ì˜¤ë¦¬ì˜¨",
        # ... ë‚˜ë¨¸ì§€ ì¢…ëª©
    }
    return stock_names.get(stock_code, stock_code)


def search_naver_news(query: str, count: int = 5) -> list[dict]:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰"""
    url = "https://search.naver.com/search.naver"
    params = {
        "where": "news",
        "query": f"{query} ì£¼ì‹",
        "sort": 1,  # ìµœì‹ ìˆœ
    }

    headers = {
        "User-Agent": random.choice(USER_AGENTS),
    }

    try:
        response = requests.get(url, params=params, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")

        news_items = []
        articles = soup.select("div.news_area")[:count]

        for article in articles:
            title_elem = article.select_one("a.news_tit")
            desc_elem = article.select_one("div.news_dsc")
            date_elem = article.select_one("span.info")

            if title_elem:
                news_items.append({
                    "title": title_elem.get_text(strip=True),
                    "url": title_elem.get("href", ""),
                    "description": desc_elem.get_text(strip=True) if desc_elem else "",
                    "date": date_elem.get_text(strip=True) if date_elem else "",
                })

        return news_items

    except Exception as e:
        print(f"Error: {e}")
        return []


def collect_all_news(stock_codes: list[str]) -> dict:
    """ì „ì²´ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘"""
    results = {}

    for i, code in enumerate(stock_codes):
        stock_name = get_stock_name(code)
        print(f"[{i+1}/{len(stock_codes)}] {stock_name} ({code})...", end=" ")

        news = search_naver_news(stock_name, count=5)
        if news:
            results[code] = {
                "stock_name": stock_name,
                "news": news,
                "collected_at": datetime.utcnow().isoformat(),
            }
            print(f"{len(news)}ê°œ ë‰´ìŠ¤")
        else:
            print("No news")

        # Rate limit
        time.sleep(random.uniform(1.5, 2.5))

    return results


def save_news_to_supabase(results: dict):
    """ë‰´ìŠ¤ ë°ì´í„° Supabase ì €ì¥ (ë˜ëŠ” ì„ì‹œ íŒŒì¼)"""
    import json

    # ì„ì‹œë¡œ JSON íŒŒì¼ì— ì €ì¥
    output_dir = Path(__file__).parent.parent / "data" / "news"
    output_dir.mkdir(parents=True, exist_ok=True)

    date_str = datetime.utcnow().strftime("%Y%m%d")
    output_file = output_dir / f"news_{date_str}.json"

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"âœ… ë‰´ìŠ¤ ì €ì¥: {output_file}")


def main():
    print("=" * 50)
    print("ğŸ“° News Collection")
    print("=" * 50)

    from collect_daily_prices import get_portfolio_stocks
    stock_codes = get_portfolio_stocks()
    print(f"ğŸ“‹ Target Stocks: {len(stock_codes)}ê°œ\n")

    results = collect_all_news(stock_codes)

    print(f"\nğŸ“° Collected news for {len(results)} stocks")
    save_news_to_supabase(results)

    print("\nâœ… News collection completed!")


if __name__ == "__main__":
    main()
